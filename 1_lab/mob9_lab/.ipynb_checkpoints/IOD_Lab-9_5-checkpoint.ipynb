{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXUiceLiLNOv"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmJwFqq5LNOx"
   },
   "source": [
    "# Demo 9.5: Working with Text\n",
    "- Using [NLTK](http://www.nltk.org) (Natural Language Toolkit)\n",
    "- Using [spaCy](https://spacy.io)\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Run the cells\n",
    "- Observe and understand the results\n",
    "- Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I83eiiJqLNO0"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.2.1-cp38-cp38-win_amd64.whl (12.2 MB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-win_amd64.whl (113 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-win_amd64.whl (452 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 wasabi-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:02.954027Z",
     "start_time": "2020-10-07T06:27:01.413950Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-ecJn_1MLNO4"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import regex as re\n",
    "\n",
    "# conda install -c conda-forge spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsSgzr8MLNO9"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:02.961095Z",
     "start_time": "2020-10-07T06:27:02.957131Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HvyU9ZbtLNPA"
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "\n",
    "input_file = '../DATA/ncc-1701-D.txt'\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KF9gBdpALNPE"
   },
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:02.968659Z",
     "start_time": "2020-10-07T06:27:02.964905Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-JImxUzrLNPG",
    "outputId": "13ce31ea-531e-4abd-8f21-c726df89bb21",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5ePf_5bLNPK"
   },
   "source": [
    "## Work the data\n",
    "- if necessary or desired\n",
    "    - remove text or content, e.g. quotes (\") or metadata (===)\n",
    "    - add content or markers, e.g. (#FLAG, --NAME--)\n",
    "    - remove or convert special symbols, e.g. \"Ã©\" to \"e\"\n",
    "    - remove or convert emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:02.974518Z",
     "start_time": "2020-10-07T06:27:02.972444Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BRq7VfoOLNPM"
   },
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMG9Yim0LNPP"
   },
   "source": [
    "## Helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:02.983629Z",
     "start_time": "2020-10-07T06:27:02.977556Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3oIPRH3gLNPQ"
   },
   "outputs": [],
   "source": [
    "# create a bar chart of the frequency of the words in the text\n",
    "def plot_words(tokens, top = 30):\n",
    "    tokens_counter = Counter(tokens)\n",
    "    tok = [t for (t, _) in tokens_counter.most_common()]\n",
    "    val = [v for (_, v) in tokens_counter.most_common()]\n",
    "\n",
    "    plt.figure(figsize = (16, 6))\n",
    "    plt.bar(tok[:top], val[:top])\n",
    "    plt.title('Number of terms: %d' % len(tokens_counter))\n",
    "    plt.xticks(rotation = 90)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1FsFC6yLNPS"
   },
   "source": [
    "## spaCy model invocation and text processing\n",
    "spaCy does the processing of the text as part of the reading of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:03.512678Z",
     "start_time": "2020-10-07T06:27:02.985594Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JgM48u-dLNPU"
   },
   "outputs": [],
   "source": [
    "# load spaCy and the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process the text\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8--KeYzLNPW"
   },
   "source": [
    "## Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:03.520458Z",
     "start_time": "2020-10-07T06:27:03.514874Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-7lVWo5DLNPe",
    "outputId": "b679dc9c-f19f-4ad2-fdbd-9b6755c17bae"
   },
   "outputs": [],
   "source": [
    "# only show the results\n",
    "# spaCy has done it already\n",
    "for i, t in enumerate(doc):\n",
    "    print('%2d| %r' % (i+1, t.text))\n",
    "    if t.text == '.':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:03.529986Z",
     "start_time": "2020-10-07T06:27:03.527209Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Gcg4Hkx4LNPh",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ANSWER - Visualise the tokenised words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAtMzEZTLNPj"
   },
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:03.540660Z",
     "start_time": "2020-10-07T06:27:03.534759Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zOzudmrILNPq",
    "outputId": "1e58fcfc-674d-42f1-e82a-63f7d61a837a"
   },
   "outputs": [],
   "source": [
    "## spaCy\n",
    "print('i | with stop words without')\n",
    "print('--| --------------- ------------')\n",
    "\n",
    "# for all the tokens\n",
    "for i, t in enumerate(doc):\n",
    "    print('%2d| %-15r %r' % (i+1, t.text, ('' if t.is_stop else t.text)))\n",
    "\n",
    "    # break after the first sentence\n",
    "    if t.text == '.':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:03.899378Z",
     "start_time": "2020-10-07T06:27:03.543763Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "B2LoqLmULNPs",
    "outputId": "63f83744-f889-462f-f166-d8add6a03f7e"
   },
   "outputs": [],
   "source": [
    "plot_words(['%r' % t.text for t in doc if not (t.is_stop | t.is_punct)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9uqME12uLNPu"
   },
   "source": [
    "### Check Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:03.906982Z",
     "start_time": "2020-10-07T06:27:03.901455Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vD-BrcLALNPz",
    "outputId": "a26b4e4f-095a-4086-a8fe-841cbbfb5b05"
   },
   "outputs": [],
   "source": [
    "## spaCy\n",
    "for i, t in enumerate(doc):\n",
    "    print('%2d|%-12r : %-5s %s' % (i+1, t.text, t.pos_, t.tag_))\n",
    "    if t.text == '.':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:03.914723Z",
     "start_time": "2020-10-07T06:27:03.909364Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "448zp9_vLNQC",
    "outputId": "561168c1-3a42-449d-9be3-dd22182459c3"
   },
   "outputs": [],
   "source": [
    "## spaCy\n",
    "print('i | Token        Lemma')\n",
    "print('--| ------------ ------------')\n",
    "for i, t in enumerate(doc):\n",
    "    print('%2d| %-12r %r' % (i+1, t.text, t.lemma_))\n",
    "    if t.text == '.':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.252720Z",
     "start_time": "2020-10-07T06:27:03.917101Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "i-7pqVMxLNQD",
    "outputId": "6deff35e-0d28-4351-bbab-7600618eb817"
   },
   "outputs": [],
   "source": [
    "plot_words(['%r' % t.lemma_ for t in doc if not (t.is_stop | t.is_punct)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHUdgHM7LNQF"
   },
   "source": [
    "### Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.308642Z",
     "start_time": "2020-10-07T06:27:04.255170Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yyOrZ42oLNQH",
    "outputId": "dfcee498-0002-4064-98ca-fa11668978d5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## spaCy\n",
    "for i, s in enumerate(doc.sents):\n",
    "    print('%2d: %s' % (i, re.sub(r'\\n+', '', s.text)))\n",
    "    if s.as_doc().ents:\n",
    "        print('-'*80)\n",
    "        for e in s.as_doc().ents:\n",
    "            print('%-11s: %s' % (e.label_, re.sub(r'\\n+', '', e.text)))\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Using NLTK for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:42:08.014090Z",
     "start_time": "2020-10-07T06:42:06.293808Z"
    }
   },
   "outputs": [],
   "source": [
    "# conda install -c anaconda nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Python interpreter, execute the following commands:\n",
    "\n",
    "- import nltk\n",
    "- nltk.download()\n",
    "\n",
    "At the NLTK Downloader GUI, install the following:\n",
    "\n",
    "- averaged_perceptron_tagger\n",
    "- ACE Named Entity Chunker (Maximum entropy)\n",
    "- Punkt Tokenizer Models\n",
    "- Stopwords Corpus\n",
    "- WordNet\n",
    "- Word Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.615658Z",
     "start_time": "2020-10-07T06:27:04.311241Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pML63M9KLNPX",
    "outputId": "29a6c627-0eee-4ece-bb34-47eacbbb53ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use nltk to find tokens\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "for i, t in enumerate(tokens[:25]):\n",
    "    print('%2d| %r' % (i+1, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.618540Z",
     "start_time": "2020-10-07T06:27:01.440Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "G5C3ituYLNPZ",
    "outputId": "db39aade-f506-4b45-ed1e-56327c19e4f7"
   },
   "outputs": [],
   "source": [
    "plot_words(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.620114Z",
     "start_time": "2020-10-07T06:27:01.441Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BnY2rFzuLNPj",
    "outputId": "f64087cd-8e4b-4df5-fb2d-7b04851bde34",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stopWords.sort()\n",
    "print(', '.join(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.621900Z",
     "start_time": "2020-10-07T06:27:01.443Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dksPJF7YcI1o"
   },
   "outputs": [],
   "source": [
    "# ANSWER - create a list of tokens withOUT the stop words\n",
    "# NOTE: see the `.lower()` method applied to token\n",
    "tokens_no_stop = '' # create a list of tokens withOUT the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.623626Z",
     "start_time": "2020-10-07T06:27:01.444Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "a82KBILRLNPm",
    "outputId": "b33cef88-79ab-4e06-9c0e-9f2a4a588306"
   },
   "outputs": [],
   "source": [
    "## NLTK\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "print('i | with stop words without')\n",
    "print('--| --------------- ------------')\n",
    "\n",
    "# for all the tokens\n",
    "while i < len(tokens):\n",
    "    # same word\n",
    "    if tokens[i] == tokens_no_stop[j]:\n",
    "        print('%2d| %-15r %r' % (i+1, tokens[i], tokens_no_stop[j]))\n",
    "        j += 1\n",
    "    # not the same word\n",
    "    else:\n",
    "        print('%2d| %-15r' % (i+1, tokens[i]))\n",
    "\n",
    "    # next word\n",
    "    i += 1\n",
    "    # break after the first sentence\n",
    "    if tokens[i-1] == '.':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.625071Z",
     "start_time": "2020-10-07T06:27:01.445Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uPtOoiwuLNPo"
   },
   "outputs": [],
   "source": [
    "# ANSWER - Create plot of words without stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.626712Z",
     "start_time": "2020-10-07T06:27:01.447Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KRupju0sLNPv"
   },
   "outputs": [],
   "source": [
    "# define PoS\n",
    "pos_list = {\n",
    "    'CC':   'coordinating conjunction',\n",
    "    'CD':   'cardinal digit',\n",
    "    'DT':   'determiner',\n",
    "    'EX':   'existential there (like: \"there is\" ... think of it like \"there exists\")',\n",
    "    'FW':   'foreign word',\n",
    "    'IN':   'preposition/subordinating conjunction',\n",
    "    'JJ':   'adjective \"big\"',\n",
    "    'JJR':  'adjective, comparative \"bigger\"',\n",
    "    'JJS':  'adjective, superlative \"biggest\"',\n",
    "    'LS':   'list marker 1)',\n",
    "    'MD':   'modal could, will',\n",
    "    'NN':   'noun, singular \"desk\"',\n",
    "    'NNS':  'noun plural \"desks\"',\n",
    "    'NNP':  'proper noun, singular \"Harrison\"',\n",
    "    'NNPS': 'proper noun, plural \"Americans\"',\n",
    "    'PDT':  'predeterminer \"all the kids\"',\n",
    "    'POS':  'possessive ending parent\"s',\n",
    "    'PRP':  'personal pronoun I, he, she',\n",
    "    'PRP$': 'possessive pronoun my, his, hers',\n",
    "    'RB':   'adverb very, silently,',\n",
    "    'RBR':  'adverb, comparative better',\n",
    "    'RBS':  'adverb, superlative best',\n",
    "    'RP':   'particle give up',\n",
    "    'TO':   'to go \"to\" the store.',\n",
    "    'UH':   'interjection errrrrrrrm',\n",
    "    'VB':   'verb, base form take',\n",
    "    'VBD':  'verb, past tense took',\n",
    "    'VBG':  'verb, gerund/present participle taking',\n",
    "    'VBN':  'verb, past participle taken',\n",
    "    'VBP':  'verb, sing. present, non-3d take',\n",
    "    'VBZ':  'verb, 3rd person sing. present takes',\n",
    "    'WDT':  'wh-determiner which',\n",
    "    'WP':   'wh-pronoun who, what',\n",
    "    'WP$':  'possessive wh-pronoun whose',\n",
    "    'WRB':  'wh-abverb where, when',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.628228Z",
     "start_time": "2020-10-07T06:27:01.448Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "asqUoCHgLNPx",
    "outputId": "6e3cab17-f736-49f3-ce62-8e759d72ac35",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## nltk\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "for i, t in enumerate(tagged[:25]):\n",
    "    print('%2d|%-12r : %-4s %s' % (i+1, t[0], t[1], (pos_list[t[1]] if t[1] in pos_list else '-')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_n9qMgyULNP1"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.629787Z",
     "start_time": "2020-10-07T06:27:01.450Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pL1vRUzxLNP2"
   },
   "outputs": [],
   "source": [
    "## nltk\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "stemmed = ' '.join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2d9a6QazLNP4"
   },
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.631193Z",
     "start_time": "2020-10-07T06:27:01.452Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "OAbriUsXLNP7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## nltk\n",
    "wl = nltk.stem.WordNetLemmatizer()\n",
    "lemma = ' '.join([wl.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.632590Z",
     "start_time": "2020-10-07T06:27:01.453Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yVIqX7HBLNP9",
    "outputId": "26f47e5f-7927-4ceb-e7aa-148b1a0d6798"
   },
   "outputs": [],
   "source": [
    "## nltk\n",
    "dot = stemmed.find('.') + 1\n",
    "sl = stemmed[:dot].split()\n",
    "dot = lemma.find('.') + 1\n",
    "ll = lemma[:dot].split()\n",
    "\n",
    "print('i | Stem           Lemma')\n",
    "print('--| -------------- ------------')\n",
    "for i, p in enumerate(zip(sl, ll)):\n",
    "    print('%2d| %-12r   %-12r' % (i+1, p[0], p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.634005Z",
     "start_time": "2020-10-07T06:27:01.455Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eG4yOgwVLNP-",
    "outputId": "70476f16-7ac6-4c40-e39f-7a6547c11fec"
   },
   "outputs": [],
   "source": [
    "plot_words(stemmed.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.635904Z",
     "start_time": "2020-10-07T06:27:01.457Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uEJR57TYLNQA",
    "outputId": "e96f9bb4-cdfc-432c-b0af-bcdb3a6c2c6c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_words(lemma.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T06:27:04.637228Z",
     "start_time": "2020-10-07T06:27:01.459Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sw6fYdgXLNQG",
    "outputId": "a83ea9db-825d-4031-8112-f022b71f565b"
   },
   "outputs": [],
   "source": [
    "## nltk\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "for e in entities:\n",
    "    s = re.sub(r'[\\(\\)]', '', str(e))\n",
    "    if s.find('/NNP') > 0:\n",
    "        t = s.split()[0]\n",
    "        n = ' '.join([re.sub(r'/NNP', '', x) for x in s.split()[1:]])\n",
    "        print('%-12s: %s' % (t, n))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IOD_Lab-9_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.976px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
